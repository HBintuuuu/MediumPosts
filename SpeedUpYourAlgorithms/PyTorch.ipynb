{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Speed_Up_Your_Algo_PyTorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "nJyq4ZJ3CL5-",
        "IOpLFtlLj_Sf",
        "zLBPSTrUkEDN",
        "ma-vz8-qCSwK",
        "70ana3V1EIFi",
        "4oxBruH9pPFV",
        "ZrV3b8zWGgoX",
        "KGE6gB_CJ4Oi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "FkDg9fic_5Rl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a part of series I am writing, named \"**Speed Up Your Algorithms**\". \n",
        "\n",
        "Here is the list of posts with files I have written:\n",
        "[Github-SpeedUpYourAlgorithms](https://github.com/PuneetGrov3r/MediumPosts/tree/master/SpeedUpYourAlgorithms)"
      ]
    },
    {
      "metadata": {
        "id": "nJyq4ZJ3CL5-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initial"
      ]
    },
    {
      "metadata": {
        "id": "IOpLFtlLj_Sf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Torch, Torchvision:"
      ]
    },
    {
      "metadata": {
        "id": "m9K9aQp_CEw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "ae864a33-41c6-4e51-db20-521f2b0f1962"
      },
      "cell_type": "code",
      "source": [
        "!cat /usr/local/cuda/version.txt"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Version 9.2.148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "srtGQ9C1AdLT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "b1e80dff-ace8-4f21-f6e3-f65907b08401"
      },
      "cell_type": "code",
      "source": [
        "!pip install http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip install torchvision"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==0.4.1 from http://download.pytorch.org/whl/cu92/torch-0.4.1-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision) (0.4.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.15.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zLBPSTrUkEDN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pycuda:"
      ]
    },
    {
      "metadata": {
        "id": "8RpbRcfdeuk7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "6f9b769c-79d7-430f-d6f1-2851b7752f5c"
      },
      "cell_type": "code",
      "source": [
        "# https://medium.com/@iphoenix179/running-cuda-c-c-in-jupyter-or-how-to-run-nvcc-in-google-colab-663d33f53772\n",
        "# https://developer.nvidia.com/cuda-90-download-archive?target_os=Linux&target_arch=x86_64&target_distro=Ubuntu&target_version=1704&target_type=deblocal\n",
        "!apt update -qq;\n",
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
        "!mv cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb  \n",
        "!dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "!apt-get update -qq;\n",
        "!apt-get install cuda gcc-5 g++-5 -y -qq;\n",
        "!ln -s /usr/bin/gcc-5 /usr/local/cuda/bin/gcc;\n",
        "!ln -s /usr/bin/g++-5 /usr/local/cuda/bin/g++;\n",
        "#!apt install cuda-9.2;"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "--2018-11-03 10:11:53--  https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 192.229.189.146\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|192.229.189.146|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb?DduKn27oFYsf9zJHRVecweSSUf3Xynx7l8DkJqlFYWECDZsz8UczpQv0g8nVfHWAN7wmMF58e3um2APBXyJ_mCJU1inU65_KKAnP0kNLRe1pmZj9iI3A5vqccCaz79TtkCQs36yROriZWKH9R2xOaI5H72kqxMY2H2ijn5A5A5i_MK4iku2TErrpnYQwrSQ-6QoFZF1SFVDu6axRBaMl [following]\n",
            "--2018-11-03 10:11:53--  https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb?DduKn27oFYsf9zJHRVecweSSUf3Xynx7l8DkJqlFYWECDZsz8UczpQv0g8nVfHWAN7wmMF58e3um2APBXyJ_mCJU1inU65_KKAnP0kNLRe1pmZj9iI3A5vqccCaz79TtkCQs36yROriZWKH9R2xOaI5H72kqxMY2H2ijn5A5A5i_MK4iku2TErrpnYQwrSQ-6QoFZF1SFVDu6axRBaMl\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.232.112, 2606:2800:247:2063:46e:21d:825:102e\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.232.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1216133170 (1.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu170 100%[===================>]   1.13G   124MB/s    in 8.0s    \n",
            "\n",
            "2018-11-03 10:12:02 (145 MB/s) - ‘cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb’ saved [1216133170/1216133170]\n",
            "\n",
            "(Reading database ... 79801 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-repo-ubuntu1704-9-0-local (9.0.176-1) over (9.0.176-1) ...\n",
            "Setting up cuda-repo-ubuntu1704-9-0-local (9.0.176-1) ...\n",
            "OK\n",
            "E: Unmet dependencies. Try 'apt --fix-broken install' with no packages (or specify a solution).\n",
            "ln: failed to create symbolic link '/usr/local/cuda/bin/gcc': File exists\n",
            "ln: failed to create symbolic link '/usr/local/cuda/bin/g++': File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pUBT0zfUFlz5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://alisonrowland.com/articles/installing-pycuda-via-pip\n",
        "# https://codeyarns.com/2015/07/31/pip-install-error-with-pycuda/\n",
        "import os\n",
        "PATH = os.environ[\"PATH\"]\n",
        "os.environ[\"PATH\"] = \"/usr/local/cuda-9.2/bin:/usr/local/cuda/bin:\" + PATH\n",
        "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda/lib64\"\n",
        "os.environ[\"CUDA_ROOT\"] = \"/usr/local/cuda/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "He-T3N_uGB8i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip -q install --ignore-installed pycuda"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ma-vz8-qCSwK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import"
      ]
    },
    {
      "metadata": {
        "id": "u0CQHYVcKjKs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch import nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWWAJMSVDjLN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pycuda.driver as cuda\n",
        "cuda.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "70ana3V1EIFi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2. How to check the availability of cuda?"
      ]
    },
    {
      "metadata": {
        "id": "aJTVCSj2A9jS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a7f421ce-360c-4775-feeb-8f600055f506"
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available() # We are on GPU instance, so cuda is already installed."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "T--ZU71EEkFr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3. How to get more info on your cuda devices?"
      ]
    },
    {
      "metadata": {
        "id": "h5ujkSp2nVOe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e7065c80-61b7-4ed5-c821-47adee37aae0"
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.current_device() # Get Id of current cuda device"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "dfyIYR9rE4Pg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b230d9ac-f936-4d0a-a10a-4c302e47d6df"
      },
      "cell_type": "code",
      "source": [
        "cuda.Device(0).name() # get name of device using pycuda"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "PdfRQs65m_7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e809e92e-15de-44ba-d32e-95275443bd0d"
      },
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_name(0) # or using torch"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "A8ALco5IngOu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d90316cd-0c5b-4d64-cf4c-5d61b52d4240"
      },
      "cell_type": "code",
      "source": [
        "# Simple class for getting info on all cuda compatible devices\n",
        "import pycuda.autoinit\n",
        "\n",
        "class aboutCudaDevices():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def num_devices(self):\n",
        "        \"\"\"Return number of devices connected.\"\"\"\n",
        "        return cuda.Device.count()\n",
        "    \n",
        "    def devices(self):\n",
        "        \"\"\"Get info on all devices connected.\"\"\"\n",
        "        num = cuda.Device.count()\n",
        "        print(\"%d device(s) found:\"%num)\n",
        "        for i in range(num):\n",
        "            print(cuda.Device(i).name(), \"(Id: %d)\"%i)\n",
        "            \n",
        "    def mem_info(self):\n",
        "        \"\"\"Get available and total memory of all devices.\"\"\"\n",
        "        available, total = cuda.mem_get_info()\n",
        "        print(\"Available: %.2f GB\\nTotal:     %.2f GB\"%(available/1e9, total/1e9))\n",
        "        \n",
        "    def attributes(self, device_id=0):\n",
        "        \"\"\"Get attributes of device with device Id = device_id\"\"\"\n",
        "        return cuda.Device(device_id).get_attributes()\n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"Class representation as number of devices connected and about them.\"\"\"\n",
        "        num = cuda.Device.count()\n",
        "        string = \"\"\n",
        "        string += (\"%d device(s) found:\\n\"%num)\n",
        "        for i in range(num):\n",
        "            string += ( \"    %d) %s (Id: %d)\\n\"%((i+1),cuda.Device(i).name(),i))\n",
        "            string += (\"          Memory: %.2f GB\\n\"%(cuda.Device(i).total_memory()/1e9))\n",
        "        return string\n",
        "\n",
        "# You can print output just by typing its name (__repr__):\n",
        "aboutCudaDevices()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1 device(s) found:\n",
              "    1) Tesla K80 (Id: 0)\n",
              "          Memory: 12.00 GB"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "BAyU2477owkl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Lets check how much memory is allocated:\n",
        "torch.cuda.memory_allocated()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Equ6KD99oEMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "62fc2021-4512-4e85-f713-92fa26c2e1a5"
      },
      "cell_type": "code",
      "source": [
        "# Let allocate some arrays to GPU:\n",
        "mat_a = torch.cuda.FloatTensor((100., 100.))\n",
        "mat_a"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([100., 100.], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "O6b6mf59oXhe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e1df177-1b67-4960-f1a4-97fc8a064a04"
      },
      "cell_type": "code",
      "source": [
        "# Lets check how muc memory is allocated now:\n",
        "torch.cuda.memory_allocated()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "Al9L1fBooqx-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bd1c006d-156b-4f58-aea4-2d40c2c27827"
      },
      "cell_type": "code",
      "source": [
        "# Memory used by Cache Allocator:\n",
        "torch.cuda.memory_cached()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1048576"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "metadata": {
        "id": "V0JshoUtDPM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# These memory methods are only available for GPUs. And that's where they are actually needed."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4oxBruH9pPFV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4. How to store Tensors and run Models on GPU?"
      ]
    },
    {
      "metadata": {
        "id": "54xGuTWvo5Z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a0bc1356-1812-4572-9ad5-59e7201dd0e9"
      },
      "cell_type": "code",
      "source": [
        "# For storing something on CPU:\n",
        "Mat_cpu = torch.FloatTensor([1., 2.])\n",
        "Mat_cpu"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "metadata": {
        "id": "4HzPhPO5EIVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4e630d86-5b80-413c-c5b3-c7ce34d67806"
      },
      "cell_type": "code",
      "source": [
        "# To put this on GPU:\n",
        "Mat_gpu = Mat_cpu.cuda()\n",
        "Mat_gpu, Mat_cpu"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1., 2.], device='cuda:0'), tensor([1., 2.]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "4EXubaSKETq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "36eab0c3-5396-452c-f306-8e4ef31bc5e2"
      },
      "cell_type": "code",
      "source": [
        "# Or you could have done:\n",
        "Mat_gpu2 = torch.cuda.FloatTensor([1., 2.])\n",
        "Mat_gpu2"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2.], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "metadata": {
        "id": "WX3ROV-vFAMG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Putting Model on GPU:\n",
        "sq = nn.Sequential(\n",
        "         nn.Linear(20, 20),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(20, 4),\n",
        "         nn.Softmax()\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oiLwGOdtGJqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = sq.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PfZlJIUUGSoJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f72686ff-ad80-438a-d5f8-c5ffc086718f"
      },
      "cell_type": "code",
      "source": [
        "# To check if model is on GPU:\n",
        "# From the discussions here: discuss.pytorch.org/t/how-to-check-if-model-is-on-cuda\n",
        "next(model.parameters()).is_cuda"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "ZrV3b8zWGgoX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5. How to select and work on GPU(s) if you have multiple of them?"
      ]
    },
    {
      "metadata": {
        "id": "g4B86rMUGbHI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ccb3379d-0cd3-4753-e7b2-d1c225388fc9"
      },
      "cell_type": "code",
      "source": [
        "# Sadly we only have one GPU here, so we will look into the dynamics with that GPU only.\n",
        "cuda0 = torch.device('cuda:0')\n",
        "\n",
        "x = torch.Tensor([1., 2.]).to(cuda0)\n",
        "x"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 2.], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "uvNDslc6IvUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# To change the default Device:\n",
        "torch.cuda.set_device(0) # '0' is the Id of device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bmsLfUk3JegG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b5744a38-5744-4a8c-9401-7a0e8fd561a3"
      },
      "cell_type": "code",
      "source": [
        "y = torch.Tensor([3., 4.]).to(cuda0)\n",
        "z = x + y\n",
        "z # This tensor is stored on the same device as x and y"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4., 6.], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "KGE6gB_CJ4Oi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6. Data Parallelism?"
      ]
    },
    {
      "metadata": {
        "id": "cyVerBqXJxwB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Again we only have one GPU, so we will only use that to look into dynamics\n",
        "module =  nn.Sequential(\n",
        "         nn.Linear(20, 20),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(20, 4),\n",
        "         nn.Softmax()\n",
        ").cuda()\n",
        "\n",
        "inp = torch.ones((30, 20)).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ik8h9tc3LTKQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "575bf4f1-3933-4d89-8c1c-0837ccd9ba44"
      },
      "cell_type": "code",
      "source": [
        "# Replicate module to devices in device_ids\n",
        "replicas = nn.parallel.replicate(module, [0])\n",
        "\n",
        "# Distribute input to devices in device_ids\n",
        "inputs = nn.parallel.scatter(inp, [0])\n",
        "\n",
        "# Apply the models to corresponding inputs\n",
        "outputs = nn.parallel.parallel_apply(replicas, inputs)\n",
        "\n",
        "# Gather result from all devices to output_device\n",
        "result = nn.parallel.gather(outputs, 0)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "9hosvC37Lp1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1cb95020-60a9-484f-da99-13bd912cd37f"
      },
      "cell_type": "code",
      "source": [
        "result.shape # Output from module"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "metadata": {
        "id": "vR4KkgxCLtWC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "50f9bfb4-1829-4634-fa67-d8626269ea27"
      },
      "cell_type": "code",
      "source": [
        "# Or we could have done:\n",
        "model = nn.DataParallel(model, device_ids=[0])\n",
        "result = model(inp)\n",
        "result.shape"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    }
  ]
}
